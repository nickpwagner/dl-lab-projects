# mode
# note that regression is quite sensitive to the optimizer (e.g. SGD with Batchsize 16 you will get exploding gradients). Adam works fine.
mode:
  value: "regression" #"binary_class" #"multi_class" # "regression", "binary_class"

# data
n_classes:
  value: 5
data_dir:
  value: "C:/DL_Lab/IDRID_dataset/" #"/content/"
val_split:
  value: 0.8
augment_crop:
  value: 450
img_width:
  value: 512

# train
epochs:
  value: 20
batch_size:
  value: 16
optimizer:
  value: "adam"
learning_rate:
  value: 0.01
learning_rate_decay:
  value: 10

# architecture
cnn_input_shape:
  value: [224, 224, 3]
architecture:
  value: "vgg16" # "resnet50"
dense0:
  value: 256
dense1:
  value: 64
dense2:
  value: 32
c_d_interface:
  value: "gap" # "flatten"
dropout:
  value: 0.1
reg_lambda:
  value: 0.01
