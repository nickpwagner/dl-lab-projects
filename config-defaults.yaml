# mode
# note that regression is quite sensitive to the optimizer (e.g. SGD with Batchsize 16 you will get exploding gradients). Adam works fine.
mode:
  value: "multi_class" #"binary_class" #"multi_class" # "regression", "binary_class"

# evaluate
evaluate_run:
  value: "stuttgartteam8/diabetic_retinopathy/1zktgvft"
  
# data
n_classes:
  value: 5
data_dir:
  value: "C:/DL_Lab/IDRID_dataset/" #"/content/"
val_split:
  value: 0.9
augment_crop:
  value: 450
img_width:
  value: 512
balancing:  # oversampling
  value: True

# train
epochs:
  value: 2
batch_size:
  value: 16
optimizer:
  value: "sgd"
learning_rate:
  value: 0.01
learning_rate_decay:
  value: 10
fine_tuning:
  value: True
fine_tuning_learning_rate:
  value: 0.0001
fine_tuning_epochs:
  value: 5

# architecture
cnn_input_shape:
  value: [224, 224, 3]
architecture:
  value: "vgg16" # "resnet50"
dense0:
  value: 256
dense1:
  value: 64
dense2:
  value: 32
c_d_interface:
  value: "gap" # "flatten"
dropout:
  value: 0.1
reg_lambda:
  value: 0.01
w_init_HeNormal:
  value: True

