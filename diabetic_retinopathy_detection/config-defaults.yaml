# mode
# note that regression is quite sensitive to the optimizer (e.g. SGD with Batchsize 16 you will get exploding gradients). Adam works fine.
mode:
  value: "multi_class" #"binary_class" #"multi_class" # "regression", "binary_class"

# evaluate
evaluate_run:
  value: "davidu/diabetic_retinopathy/9cf0hr55"
  #"davidu/diabetic_retinopathy/49d802at" # run12 - lemon-sweep-8 - balanced training dataset
  #"davidu/diabetic_retinopathy/9cf0hr55" # run12 - daily-sweep-8 - balanced training dataset
  #"davidu/diabetic_retinopathy/1cge9ja4" # wise-sweep-30
  #"davidu/diabetic_retinopathy/hdalq4ty" #proud-sweep-9 
  #"stuttgartteam8/diabetic_retinopathy/1zktgvft"
  
# data
n_classes:
  value: 5
data_dir:
  value: "C:/DL_Lab/IDRID_dataset/" #"/content/"
val_split:
  value: 0.9
augment_crop:
  value: 450
img_width:
  value: 600
balancing:  # oversampling
  value: True

# train
epochs:
  value: 50
batch_size:
  value: 32
optimizer:
  value: "sgd"
learning_rate:
  value: 0.01
learning_rate_decay:
  value: 10
momentum:
  value: 0.0
fine_tuning:
  value: False
fine_tuning_learning_rate:
  value: 0.0001
fine_tuning_epochs:
  value: 5

# architecture
cnn_input_shape:
  value: [224, 224, 3]
architecture:
  value: "vgg16" # "resnet50"
dense0:
  value: 2048
dense1:
  value: 1024
dense2:
  value: 0
c_d_interface:
  value: "gap" # "flatten"
dropout:
  value: 0.1
reg_lambda:
  value: 0.01
w_init_HeNormal:
  value: False

