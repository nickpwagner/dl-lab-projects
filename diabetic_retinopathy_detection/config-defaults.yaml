# mode
# note that regression is quite sensitive to the optimizer (e.g. SGD with Batchsize 16 you will get exploding gradients). Adam works fine.
mode:
  value: "multi_class" # select from "multi_class", "regression", "binary_class" # regression not supported, yet!!!

# evaluate
evaluate_run:
  value: "davidu/diabetic_retinopathy/u5ltosw6"  # multi sweep 101 - leafy-sweep-11
  #"davidu/diabetic_retinopathy/c0kshl58"  # multi sweep 101 - proud-sweep-31
  #"davidu/diabetic_retinopathy/z89dlvr4"  # binary sweep 200 - different-sweep-13
  
# data
n_classes:
  value: 5
data_dir:
  value: "C:/DL_Lab/IDRID_dataset/" #"/content/"
val_split:
  value: 0.8
augmentation:
  value: "strong"  # select from "none", "weak", "strong"
augment_crop:
  value: 550
img_width:
  value: 600
balancing:  # oversampling
  value: True

# train
epochs:
  value: 50
batch_size:
  value: 2
optimizer:
  value: "sgd"
learning_rate:
  value: 0.01
learning_rate_decay:
  value: 10
momentum:
  value: 0.0
fine_tuning:
  value: False
fine_tuning_learning_rate:
  value: 0.0001
fine_tuning_epochs:
  value: 0

# architecture
cnn_input_shape:
  value: [224, 224, 3]
architecture:
  value: "vgg16" # "resnet50"
base_model_trainable:
  value: False
dense0:
  value: 2048
dense1:
  value: 1024
dense2:
  value: 0
c_d_interface:
  value: "gap" # "flatten"
dropout:
  value: 0.1
reg_lambda:
  value: 0.01
w_init_HeNormal:
  value: False

